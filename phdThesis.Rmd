--- 
title: "Italian as Second Language in native Czechs and Slovaks"
subtitle: "From the development of a learner corpus towards a theoretical investigation"
author: "Marco Petolicchio"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: scrbook
lot: yes
lof: yes
classoption: [a4paper,twoside,12pt,chapterprefix=false, bibliography=totocnumbered,listof=flat]
bibliography: [bibliography.bib]
biblio-style: apa
link-citations: yes
github-repo: p-marco.github.io/phd-thesis
description: "My PhD thesis"
always_allow_html: yes
---

# Preface {-}
## Abstract {-}


The main topic of this doctoral dissertation is on the analysis of syntactic structures in language acquisition,  specifically in the domain of Czech and Slovak learners which acquire the Italian language. In particular, I will focus on the complex noun phrase subdomain, showing the compositionality of the phrase structure and the hierarchical fashion of this component. 
The analysis is casted in the Minimalist-oriented framework of the Generative Grammar [@chomsky1995; @chomsky1998; @chomsky2013; @hcf2002] and its application in the field of the second language acquisition [@rothmanslabakova2017;  @slabakovalealliskin2014].

The usage of an established computational ground to conduct the work, where the data retrieved by fieldwork is stored in a coherent corpus which easily permits to be queried and interpolated for the research purposes, represents a standpoint for this research in its totality, yielding for a data-based approach to the whole process. 
The annotation schema of the data is standardized in order to adhere to the major point of discussion into the discipline [@clark2010; @kueblerzinsmeinster2015; @kurdi2016], representing the plus to furnish a data source which is independent to the merely contingent purposes.

This research aims to offer a way to investigate how second language acquisition can be seen grounding on a coherent set of data in terms of annotation schema: it does insist either on the speculative questions both on computational models involved.





## Keywords {-}
Computational Linguistics; Syntax; Second Language Acquisition; Italian L2; Corpus Linguistics.


<!--chapter:end:index.Rmd-->

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(RCurl)

read.csv.orig = read.csv

read.csv = function(file, ...) {
  if (is.character(file)) {
    if (grepl('^https://', file)) {
      data = getURL(file, ssl.verifypeer=0L, followlocation=1L)
      return (read.csv.orig(text=data, ...))  
    } else if (grepl('^http://', file)) {
      data = getURL(file)
      return (read.csv.orig(text=data, ...)) 
    } else {
      return (read.csv.orig(file, ...))
    }
  } else {
    return (read.csv.orig(file, ...))
  }
}
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

# Load CSV data
czechit_data <- read.csv('https://github.com/Czech-IT/Czech-IT.github.io/raw/master/_data/dataset.csv', header=TRUE, row.names=1) 

# Create a dataframe
czechit_dataset <- data.frame(x=czechit_data, check.names=TRUE)

# Count items by accessing the row by its order
# ! R has troubles with column named with a "-" as in "item-id" 
czechit_datasetCountTokens <- rowSums(czechit_dataset[ ,8, drop = FALSE])

czechit_datasetItems <- unique(czechit_dataset[ ,0, drop = FALSE])

# Count unique values for learners
# ! It could be done also for rows, passing the argument of relevant col into the expression. For now it doesn't run so item Count is done by counting rows (which works)

library(data.table)

czechit_datasetCountLearners <- setDT(czechit_dataset)[, .(count = uniqueN(czechit_dataset[ ,1, drop = FALSE]))]



```

<!--chapter:end:05-rscripts.Rmd-->

\mainmatter

# Introduction

The main question of this thesis yields a twofold mindset that is not a corollary of the research but represents the process in which the work was conducted: how could I investigate a particular area of the language faculty as language acquisition in a way which can gain from the usage of the digital instruments in order to ground the theoretical analysis on actual data?

The idea under this research moves across the motivation to investigate over an empirically-grounded path the strategies shown by the learners during the acquisition of second languages, using an established coherent digital architecture. 
My task is twofold: on one side this provides for the developing of a  theoretically-grounded framework to research in the fields of Second Language Acquisition (SLA), while on the other this necessitates to develop a linguistic corpus which collects into a coherent fashion a set of data that represent some spotted linguistic fact in order to give a transparent documentation of the learning path.
The usage of the modern tools in developing a linguistic corpus yields for a fully documentable research path, in which is possible to reconstruct the steps and the choices which underlie its development, the methods used in the analysis, the correctness of the outcomes.
This kind of research is intimately multidisciplinar in nature, embracing different approaches and areas of interest: digital humanities, corpus and computational linguistics for the development of the linguistic corpus, general and theoretical linguistics, studies on SLA and interlanguage for the theoretical analysis. 

This introductive chapter collects a preliminary way to represent the main areas of the research, the methods involved in the analysis and the possible outcomes of such a way to conduct the work.








## Background for the thesis

Corpus Linguistics is a field of approaches developed during the last decades in order to give an empirical support to the investigations on language use and variation. It can offer strong support for analyzing the systematics which underlies the variations among the language use, yielding for empirical and quantitative methods.

> In fact, at one level it can be regarded as primarily a methodological approach:
>
> * it is empirical, analyzing the actual patterns of use in natural texts;
> * it utilizes a large and principled collection of natural texts, known as a “corpus”, as the basis for analysis;
> * it makes extensive use of computers for analysis, using both automatic and interactive techniques;
> * it depends on both quantitative and qualitative analytical techniques 
> [@biber1998]


The main tenets of such a discipline still permit to obtain different level of information starting from the texts and their annotations, to result in a general picture of the language variation. A part of this is due to a widespan documentation which overpasses the recognized linguistic theories - under the *corpus-driven* approach. 
On the other, the *corpus-based* approach permits to ground the hypothesis on a real actual set of data constitutes by language use in an empirically based way.










### Corpus-based approach: motivations for the thesis

While a strong opposition between the way to approach the corpora can be fairly molded during the actual analysis of the data in a softer manner, it can be useful to stand up and recognize those models to threat linguistic data as a two different standpoints to keep in mind for the different purposes they grow on:

* **Corpus-based**  
When a general theory on some linguistic fact is tested against a corpus in order to verify the hypotheses. 
This kind of approach is more *deductive*, while it goes top-down, proceeding from a general statement (the theory) towards a specific environment (the corpus).

* **Corpus-driven**  
Corpus-driven approach tends to proceed from the analysis of the partial specific pieces (the corpus), in order to result into a general picture (the theory).
This method is more *inductive*, going bottom-up.

Different views were proposed to face or embrace the corpora in language studies  amongst the scholars. The first one is a well-known citation by [Noam Chomsky](https://en.wikipedia.org/wiki/Noam_Chomsky), which substantially regrets any importance to corpora for a theory-oriented language modeling: 



>Any natural  corpus  will  be  skewed. 
Some  sentences  won’t  occur because  they  are  obvious,  others  because they  are  false,  still  others because  they  are  impolite. 
The  corpus,  if  natural,  will  be  so  wildly skewed that the description would be no more than a mere list.
>[Chomsky 1962, *A transformational approach to syntax* in @togninibonelli2001]


On the other hand, [Charles Fillmore](https://en.wikipedia.org/wiki/Charles_J._Fillmore) recognizes a structural place to corpora usage into language reflection:

>I have two main observations to make. The first is that I don't think there can be any corpora, however large, that contain information about all of the areas of English lexicon and grammar that I want to explore; all that I have seen are inadequate. The second observation is that every corpus that I've had a chance to examine, however small, has taught me facts that I couldn't imagine finding out about in any other way. 
>[@fillmore1992]


As in Fillmore's quotation, it appears that the distinction between deductive and inductive method cannot be really disentangled in some part of the research planning, moreover in the case when the one which is developing a corpus is the same that is going to write an analysis based on: a simple scan of the data can yields for a purpose of a general theory which needs to be refined on the real data in a more euristic manner. 
In this sense, while a *corpus-based* approach aims to generalize a picture *before* than the actual recognition of the data and the dataset takes place, it can be possible to softener a bit this difference amongst these models keeping in mind the perspective of corpus-developing related issues.

In the subsequent parts of the thesis I will try to show how the way to develop a linguistic corpus has a certain degree of influence for the successive part of research activities, and how a purely *corpus-based* method could not be apply if the research is conducted by the same person which started to collect the data.  




### Learners corpora of Italian L2: an overview

In this section I am going to summarize the most representative Italian L2 learner corpora available online, including Czech-IT, which I have co-founded since July, 2017. I will present all the relevant information and discuss the central topics of the project in a dedicate part of the thesis, while for now I list the most evaluable corpora for studying Italian as 2nd language: 

* **[GranVALICO](http://www.valico.org/valico_b_CORPUS.html)** and **[VALICO](http://www.valico.org/valico_CORPUS.html)** [@valico]  
Learner corpora provided by Turin University. They represent the most valuable sources of Italian L2 corpora. They are composed by written texts composed by the students which have the assignment to describe the vignettes provided by the teachers. The corpora are accessible online with an advanced search that permits to filter the data by different parameters (e.g. learners' L1 and education, assignments etc.). 

* **[MERLIN](http://merlin-platform.eu)** [@merlin]  
The MERLIN Corpus represents a wide-range multilingual documented resource which collects 2.286 texts written by learners of Czech, Italian and German. 
Started in 2012, the main objective is to show the different levels of acquiring languages by the usage of written texts, relying on the CEFR level schema on L2 acquisition.
The Italian-L2 subcorpus contains 813 texts.

* **[LIPS](http://parlaritaliano.it/index.php/en/data/653-corpus-lips)** [@lips]    
The corpus contains the transcriptions of more than 2000 audio files by CILS - Certificazione di Italiano come Lingua Straniera (CILS) at the Università per Stranieri of Siena between the years 1993--2006.
With more than 700k of words divided in *monologues* and *dialogues* between the candidate and the examiner, it represents one of the biggest corpora of Italian L2. The corpus is POS annotated using the tool Treetagger [@schmid1994].  

* **[Czech-IT](http://czech-it.github.io)** [@czech-it]  
The Czech-IT corpus contains chat messages, emails, coversations, surveys and assignments by more than `r toString(round((as.integer(czechit_datasetCountLearners/10))*10))` Czech and Slovak learners of Italian language. 
Started in 2017, it is fully accessible online while the data acquisition continues. The whole dataset is fully interrogable by an interactive interface and released with a Creative Commons license; POS and automatic tagging are in tune. 

* **[Corpus Italiano scritto L2](http://www.parlaritaliano.it/index.php/it/corpora-di-parlato/662-corpus-italiano-scritto-l2)** [@vogheraturco2010]  
The corpus retains 227 written texts by undergraduate students of different native languages, which study Italian as a foreign language for their courses at the University of Greenwich. 
Learners' L1 are: albanian, bosniac, chinese, french, greek, english, norwegian, portuguese, spanish, tigrinya.  
The type of texts are: *descriptive*, *narrative* and *argumental*.
The texts are syntactically annotated and the tagset is available in xml format.  


| Corpus     					| L1      | Texts 			| Tokens  	| Lemma  | Years        	|
|-------------------------------|---------|----------------:|----------:|-------:|-----------------:|
| GranVALICO 					| Various | 4778	  		|  784217 	| 13057  | 2002--2007   	|
| VALICO     					| Various | 2502  			|  382098 	|  6935  |              	|
| LIPS       					| Various | 2198  			| > 700000	|        | 1993--2006   	|
| MERLIN     					| Various |  813	        |         	|        | 2012--?      	|
| Czech-IT   					| cs,sk   | `r toString(nrow(czechit_dataset))`|`r toString(sum(czechit_datasetCountTokens))`	|        | 2017--present	|
| Corpus Italiano Scritto L2   	| Various |  227    		|   22931 	|        | 2010?			|

Table: Size of Italian L2 Corpora











## Objectives of the thesis

The three main objectives of this thesis are methodological, empirical and theoretical.

1. **Methodological objectives**  
To address the decisions and the methods raised by the compilation, the storage and the design of a learner based corpus, exploring the effective procedures for retrieving the relevant features for the analysis;
1. **Empirical objectives**  
To explore the previous generalizations of the acquisitional path in SLA literature comparing with the amount of linguistic productions given by different learners;
1. **Theoretical objectives**  
To describe the features which are relevant for characterise the language variety effect and the place of interlanguage.











### Methodological objectives



While usually seen as a sussidary tool for linguistic investigations, corpus linguistics can be regarded with a certain degree of indipendence by such aims [@sinclair2005; @sinclairCarter2004], and involves highly specialized sectors for what concerns the planning, the mantaining, the design and the scalability of the corpora.

[//]: # (Moreover, alongside the way to approach the design of the corpus in its fundamental directions, a focus is given to the methods raised in the data analysis activity and its role in terms of reproducibility.)   




[//]: # (#### Planning the corpus)


The Czech-IT corpus is composed by different kind of texts in order to exhibit the variation in language use across different communicative situations:

* Email subcorpus for the (quasi-) bureaucratic and academic language;
* SMS and other direct platforms for textual messaging for informal situations;
* Spoken discourse analysis for spontaneous modality;
* Online surveys created for obtaining auto-evaluation by learners about their acquisition: the tests are made by a certain amount of questions and tiny writing samples.


`r toString(czechit_datasetCountLearners)` are the learners inserted in the corpus. Informations about the learners concern the education level, the age group, the level of their italian knowledge, and other known languages - while their real identities are preserved by the assignment of an alpha-numeric ID.


[//]: # (#### Reproducible research)









### Empirical objectives


Amongst many scholar the role of the native language (L1) has been raised as a factor of possible conditionation in the way which the target language (L2) is acquired during the learning path: an emblematic case is the *transfer* of the knowledge about the structures of the L1 to the target, revealing the intermediate steps of the acquisitional path defined with the term *interlanguage* [@selinker1972], that we can refer as to **Interlanguage Hypothesis** (IlH). Different from this hypothesis --which recognizes a central place to the native language in the acquisitional path-- is the **Monitor Model** [@krashen1981], a multi-focal perspective on language acquisition where different factors are described as involved in the process and where the L1 could not represent that conditionation.   

Since the last 20 years, a considerable part of linguistic activity is involved in developing some sort of models to describe how the faculty of language can work, in its biological [@hcf2002], computational [@fodor2001] and cognitive components in a highly interdisciplinary environment.
Studies on SLA is a fertile field, which relies on comparative and contrastive analyses of linguistic phenomena, either both from an applied view [@ellis1994] than by theoretically grounded perspective focused on Generative framework (GenSLA) [@guasti2002; @hawkins2001; @rothmanslabakova2017; @sorace2011]. 
In this sense appears that the adoption of a general picture in which analysing the variation in grammar into a *parametric* model [@chomsky1995] can be suitable for long-standing researches on SLA and interlanguage. 

The dataset used in this thesis aims to display either the different linguistic outcomes in a wide range of communicative situations by the same learner, both than a sociolinguistic grained analysis where the variety of educational or age range can show different linguistic behaviors in the range of learners' variety.  

















### Theoretical objectives


From a theoretical viewpoint, the research is inserted in the current theories that rely on the hierarchical functioning of the language faculty, for which the variation among languages are reconducted to a parametrizing of choice amongst the languages [@adger2013; @chomsky1995; @chomsky1998; @chomsky2013; @chomsky2015; @rizzi2013], which are structurally constant, despite of the wideness of the linguistic variation: 

>We are concerned, then, with states of the language faculty, which we understand to be some array of cognitive traits and capacities, a particular component of the human mind/brain. The language faculty has an initial state, genetically determined; in the normal course of development it passes through a series of states in early childhood, reaching a relatively stable steady state that undergoes little subsequent change, apart from the lexicon. To a good first approximation, the initial state appears to be uniform for the species. 
>[@chomsky1995]

This view permits on one side to compare the syntactic structures in a coherent and schematic way, while on the other it concentrates moreover on the hierarchical fashion of the language faculty than on the linear order displayed by the utterances [@kayne1994; @moro2000]. In this perspective is generally assumed that the hierarchical phrase structure plays a central role in syntactic computation, while the *flattering* of such structures into a mono-dimensional workspace is a matter of externalization constraints and interface conditions (e.g. the need to give an ordered array where every item of the sentence is present at one time in order to be spelled out). 
I will summarize this in a representational way with the usual tree-diagram in Fig.\@ref(fig:tree1). 


```{r tree1, echo=FALSE, engine='tikz', out.width='70%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'svg', fig.cap='Structural representation of a simple sentence', engine.opts = list(template = "latex/tikz2pdf.tex"), fig.align='center'}

\begin{tikzpicture}
\tikzset{every tree node/.style={align=center,anchor=north}}
\Tree 
[ .CP \node(C){C};
[ .TP [.DP\textsubscript{i} \edge[roof]; \node(subj){The girl\\$[${\sc P:3}$]$\\$[${\sc N:sg}$]$}; ]  
[ .T\textsuperscript{1} [ .T \node(t0){eats\\$[${\sc P:3}$]$\\$[${\sc N:sg}$]$};  ]
[ .vP [ .DP\textsubscript{i} \node(ti){$t$}; ] 
[ .v\textsuperscript{1} [.DP\textsubscript{j} \edge[roof]; \node(obj){the apple}; ]  
[ .v\textsuperscript{1} [ .v \node(v){eat}; ]
[ .VP [ .V \node(tV){$\sqrt{eat}$}; ]
[ .DP\textsubscript{j} \node(tj){$t$}; ] ] ] ] ] ] ] ]
\draw[thin,<-] (subj.south) to [bend right=60] (ti.south);
\draw[thin,<-] (obj.south) to [bend right=60] (tj.south);
\end{tikzpicture}
```

Given this way to proceed, that assures a coherent framework to compare languages in a parametric way, the main theoretical question addressed here concerns the relevance and the potential usage of this perspective in the analysis of a dynamic system as during the acquisitional path and the strategies raised up by learners during the various steps in the interlanguage.








## Outline of the thesis


The first year is dedicated to the setting-up of the corpus, with the starting operations to acquire the data and elaborate a coherent way to annotate the texts with a standard schema. During the second year the corpus is planned to grow up for reach a significance level of >15000 words in order to provide quantitative analyses.
Third and fourth year will be spent in developing the theoretical analyses and refining the informatic architecture of the project, evolving in a user-friendly and interrogable way to dispense the data.  The theoretical outcome constitues the main topic of the research.

**Chapter 2** introduces ...

**Chapter 3** introduces ...

**Chapter 4** introduces ...

**Chapter 5** introduces ...


<!--chapter:end:10-introduction.Rmd-->

# Chapter 1

prova

<!--chapter:end:21-chapter1.Rmd-->

\backmatter

# Backmatter 



## Colophon 

This document is typeset with LaTeX using a custom template based on KOMA-script SCRBOOK class. The layout is based on a standard A4 paper (210 x 297mm), with 40mm margins and 10mm of binding offset.

The typesetting software used the XeTeX engine and the text is set in the open source IBM Plex font family -- in Serif, Sans Serif and Monospace variants.


## Credits 



This project is constituted by files written in Markdown syntax and exported either as a standalone website both as printer-ready product. This is due to the awesome work of the people behind different libraries:

* [Pandoc](https://bookdown.org)
* [Bookdown](https://bookdown.org) 
* [RMarkdown](https://bookdown.org) and [R](https://bookdown.org) environment.

As well, for the computational infrastructure, some tools have been used:

* [NLTK](https://bookdown.org)
* UDPIPE
* SciPY 





## About the author 

I am a Graduate Researcher involved in a Ph.D. Program in Italian Linguistics at the Department of Romance Studies in the Faculty of Philosophy at Palacky University in Olomouc, Czech Republic.

My interests span across digital humanities, syntax theories and computational linguistics.

Feel free to write me at [marco.petolicchio@gmail.com](mailto:marco.petolicchio@gmail.com) or visit [marcopetolicchio.com](http://marcopetolicchio.com) for the detailed contact list.




## Progress in the repository 

This graph represents the addition and deletion amount in the files of the project in function of time.

```{r fig.width=7, fig.height=4, echo=FALSE, message=FALSE}

library(ggplot2)
library(knitr)

options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)



dat <- read.csv("gitstats.csv", header=FALSE)
dat[is.na(dat)] <- 0
mat <- as.matrix(dat)

times <- mat[,3]
fileChanged <- mat[,5]
adds <- mat[,6]
dels <- mat[,7]

times <- as.Date(times)

prova <- ggplot(dat, aes(x=times, y=cumsum(adds))) + geom_line()


dels <- as.integer(dels) * -1
diffs <- as.integer(adds) + as.integer(dels)
prova2 <- ggplot(dat, aes(x=times, y=diffs)) + geom_line()




out <- ggplot() + geom_point(data=dat, aes(x=times, y=cumsum(adds), color = "Additions")) + geom_point(data=dat, aes(x=times, y=cumsum(dels), color = "Deletions"))  + geom_line(data=dat, aes(x=times, y=cumsum(diffs), color = "Difference")) + geom_point(data=dat, aes(x=times, y=cumsum(fileChanged), color = "Files Changed")) 

#+ xlab('Dates') + ylab('Edits')

print(out)

#kable(dat[,3:7])

```

<!--chapter:end:90-backmatter.Rmd-->

